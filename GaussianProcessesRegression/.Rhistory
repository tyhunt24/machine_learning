library(caTools)
matrix_data <- data.matrix(data)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
matrix_data <- data.matrix(data)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
matrix_data <- data.matrix(data)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(matrix_data)
matrix_data <- data.matrix(data.df)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(matrix_data)
matrix_data <- data.matrix(data.df)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
det(cor_matrix)
det(cor_matrix)
det(cov_matrix)
det(cor_matrix)
det(data.df)
knitr::opts_chunk$set(echo = TRUE)
A = as.matrix(data.frame(c(4,7,-1,8), c(-5,-2,4,2), c(-1,3,-3,6)))
A
A.svd <- svd(A)
A.svd
A.svd <- svd(A)
A.svd
# Taken off of the geeks for geeks website, just made it simpiler to explain
library(MASS)
library(tidyverse)
library(caret)
theme_set(theme_classic())
# Load the data
data("iris")
# Split the data into training (80%) and test set (20%)
set.seed(123)
training.individuals <- iris$Species %>%
createDataPartition(p = 0.8, list = FALSE)
train.data <- iris[training.individuals, ]
test.data <- iris[-training.individuals, ]
# Estimate preprocessing parameters
preproc.parameter <- train.data %>%
preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
train.transform <- preproc.parameter %>% predict(train.data)
test.transform <- preproc.parameter %>% predict(test.data)
# Fit the model
model <- lda(Species~., data = train.transform)
# Make predictions
predictions <- model %>% predict(test.transform)
# Model accuracy
mean(predictions$class==test.transform$Species)
model <- lda(Species~., data = train.transform)
model
> library(pixmap)
library(pixmap)
install.packages("pixmap")
library(pixmap)
mtr <- read.pnm('MtRush.pgm')
library(NMF)
install.packages("NMF")
library(NMF)
library(NMF)
library(NMF)
install.packages("Biobase")
library(NMF)
knitr::opts_chunk$set(echo = TRUE)
matrix_data <- data.matrix(data.df)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
# Load PCA related packages
library(stats)
library(modeldata)
library(dplyr)
library(psych)
library(rela)
library(MASS)
library(parallel)
library(creditmodel)
library(caTools)
matrix_data <- data.matrix(data.df)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
## Look up what Barlet Test and KMO test Actually mean
## Find the Statistical Significance of the Barlet Test
# Explain why it is significant
matrix_data <- data.matrix(data.df)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
## Look up what Barlet Test and KMO test Actually mean
## Find the Statistical Significance of the Barlet Test
# Explain why it is significant
det(cor_matrix)
# Explain what all of this means
View(iris)
sample <- sample.split(data.df$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
train <- subset(data.df, sample==TRUE)
test <- subset(data.df, sample == FALSE)
head(train)
# The variable jobLevel can be changed to anything that we want just make to sure to change the variables you want in the Training set as well.
model <- lm(JobLevel ~, data = train)
# The variable jobLevel can be changed to anything that we want just make to sure to change the variables you want in the Training set as well.
model <- lm(JobLevel ~., data = train)
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
train <- subset(data, sample==TRUE)
test <- subset(data, sample == FALSE)
head(train)
# The variable jobLevel can be changed to anything that we want just make to sure to change the variables you want in the Training set as well.
model <- lm(JobLevel ~., data = train)
data <- read.csv("Employee.csv", header = TRUE, sep=",")
head(data)
data$Attrition <- as.numeric(factor(data$Attrition))
data$BusinessTravel <- as.numeric(factor(data$BusinessTravel))
data$Department  <- as.numeric(factor(data$Department))
data$EducationField <- as.numeric(factor(data$EducationField))
data$Gender <- as.numeric(factor(data$Gender))
data$JobRole <- as.numeric(factor(data$JobRole))
data$MaritalStatus <- as.numeric(factor(data$MaritalStatus))
data$Over18 <- as.numeric(factor(data$Over18))
data$OverTime <- as.numeric(factor(data$OverTime))
data$Attrition <- as.numeric(factor(data$Attrition))
head(data)
data.df <- scale(data)
head(data)
data <- scale(data)
head(data)
#sum(is.na(data.df)) # data values that have NA
sum(is.null(data)) # Data values that just have nothing in them
data <- subset(data, select = -c(9, 22, 27))
#See if there are any more NA values
sum(is.na(data))
head(data)
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
sample <- sample.split(JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
sample <- sample.split(data[['JobLevel']], SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
data <- as.data.frame(scale(data)) # we have to do this because it will give us atomic variables back if we do not.
head(data)
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
train <- subset(data, sample==TRUE)
test <- subset(data, sample == FALSE)
head(train)
# The variable jobLevel can be changed to anything that we want just make to sure to change the variables you want in the Training set as well.
model <- lm(JobLevel ~., data = train)
summary(model)
prinComponents <- prcomp(data)
prinComponents <- prcomp(data)
prinComponents
prinComponents <- prcomp(data, cor=TRUE) # we do need to use the scale because we have already scaled the data.
prinComponents
prinComponents <- prcomp(data) # we do need to use the scale because we have already scaled the data.
prinComponents
loadings(prinComponents)
prinComponents <- prcomp(data) # we do need to use the scale because we have already scaled the data.
prinComponents
# Need to look up and write about what this is doing.
loadings(prinComponents)
plot(prinComponents)
plot(prinComponents)
biplot(prinComponents)
knitr::opts_chunk$set(echo = TRUE)
data <- as.data.frame(scale(data)) # we have to do this because it will give us atomic variables back if we do not.
head(data)
tail(data)
# We need to use the data set size to and cortest.barlett() function with the dataset size.
cortest.barlett(cor_matrix, n = 1400)
# Load PCA related packages
library(stats)
library(modeldata)
library(dplyr)
library(psych)
library(rela)
library(MASS)
library(parallel)
library(creditmodel)
library(caTools)
# We need to use the data set size to and cortest.barlett() function with the dataset size.
cortest.barlett(cor_matrix, n = 1400)
# We need to use the data set size to and cortest.barlett() function with the dataset size.
cortest.bartlett(cor_matrix, n=1300)
knitr::opts_chunk$set(echo = TRUE)
# Load PCA related packages
library(stats)
library(modeldata)
library(dplyr)
library(psych)
library(rela)
library(MASS)
library(parallel)
library(creditmodel)
library(caTools)
# reading the employee.csv file, it is our main data file which we are using for analysis.
data <- read.csv("Employee.csv", header = TRUE, sep=",")
head(data)
data$Attrition <- as.numeric(factor(data$Attrition))
data$BusinessTravel <- as.numeric(factor(data$BusinessTravel))
data$Department  <- as.numeric(factor(data$Department))
data$EducationField <- as.numeric(factor(data$EducationField))
data$Gender <- as.numeric(factor(data$Gender))
data$JobRole <- as.numeric(factor(data$JobRole))
data$MaritalStatus <- as.numeric(factor(data$MaritalStatus))
data$Over18 <- as.numeric(factor(data$Over18))
data$OverTime <- as.numeric(factor(data$OverTime))
data$Attrition <- as.numeric(factor(data$Attrition))
head(data)
data <- as.data.frame(scale(data)) # we have to do this because it will give us atomic variables back if we do not.
head(data)
#sum(is.na(data.df)) # data values that have NA
sum(is.null(data)) # Data values that just have nothing in them
data <- subset(data, select = -c(9, 22, 27))
#See if there are any more NA values
sum(is.na(data))
head(data)
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
train <- subset(data, sample==TRUE)
test <- subset(data, sample == FALSE)
head(train)
# The variable jobLevel can be changed to anything that we want just make to sure to change the variables you want in the Training set as well.
model <- lm(JobLevel ~., data = train)
summary(model)
cor_matrix <- cor(data.df)
cov_matrix <- cov(data.df)
cor_matrix
cov_matrix
matrix_data <- data.matrix(data.df)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
## Look up what Barlet Test and KMO test Actually mean
## Find the Statistical Significance of the Barlet Test
# Explain why it is significant
# Load PCA related packages
library(stats)
library(modeldata)
library(dplyr)
library(psych)
library(rela)
library(MASS)
library(parallel)
library(creditmodel)
library(caTools)
data <- read.csv("Employee.csv", header = TRUE, sep=",")
head(data)
data$Attrition <- as.numeric(factor(data$Attrition))
data$BusinessTravel <- as.numeric(factor(data$BusinessTravel))
data$Department  <- as.numeric(factor(data$Department))
data$EducationField <- as.numeric(factor(data$EducationField))
data$Gender <- as.numeric(factor(data$Gender))
data$JobRole <- as.numeric(factor(data$JobRole))
data$MaritalStatus <- as.numeric(factor(data$MaritalStatus))
data$Over18 <- as.numeric(factor(data$Over18))
data$OverTime <- as.numeric(factor(data$OverTime))
data$Attrition <- as.numeric(factor(data$Attrition))
head(data)
# Load PCA related packages
library(stats)
library(modeldata)
library(dplyr)
library(psych)
library(rela)
library(MASS)
library(parallel)
library(creditmodel)
library(caTools)
data <- read.csv("Employee.csv", header = TRUE, sep=",")
head(data)
data$Attrition <- as.numeric(factor(data$Attrition))
data$BusinessTravel <- as.numeric(factor(data$BusinessTravel))
data$Department  <- as.numeric(factor(data$Department))
data$EducationField <- as.numeric(factor(data$EducationField))
data$Gender <- as.numeric(factor(data$Gender))
data$JobRole <- as.numeric(factor(data$JobRole))
data$MaritalStatus <- as.numeric(factor(data$MaritalStatus))
data$Over18 <- as.numeric(factor(data$Over18))
data$OverTime <- as.numeric(factor(data$OverTime))
data$Attrition <- as.numeric(factor(data$Attrition))
head(data)
data <- as.data.frame(scale(data)) # we have to do this because it will give us atomic variables back if we do not.
head(data)
tail(data)
#sum(is.na(data.df)) # data values that have NA
sum(is.null(data)) # Data values that just have nothing in them
data <- subset(data, select = -c(9, 22, 27))
#See if there are any more NA values
sum(is.na(data))
head(data)
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
train <- subset(data, sample==TRUE)
test <- subset(data, sample == FALSE)
head(train)
# The variable jobLevel can be changed to anything that we want just make to sure to change the variables you want in the Training set as well.
model <- lm(JobLevel ~., data = train)
summary(model)
cor_matrix <- cor(data.df)
cov_matrix <- cov(data.df)
cor_matrix
cov_matrix
matrix_data <- data.matrix(data.df)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
## Look up what Barlet Test and KMO test Actually mean
## Find the Statistical Significance of the Barlet Test
# We need to use the data set size to and cortest.barlett() function with the dataset size.
cortest.bartlett(cor_matrix, n=1300)
matrix_data <- data.matrix(data)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
## Look up what Barlet Test and KMO test Actually mean
## Find the Statistical Significance of the Barlet Test
# We need to use the data set size to and cortest.barlett() function with the dataset size.
cortest.bartlett(cor_matrix, n=1300)
# We need to use the data set size to and cortest.barlett() function with the dataset size.
cortest.bartlett(cor_matrix, n=1300)
cor_matrix <- cor(data.df)
cov_matrix <- cov(data.df)
cor_matrix
cov_matrix
matrix_data <- data.matrix(data)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
## Look up what Barlet Test and KMO test Actually mean
## Find the Statistical Significance of the Barlet Test
# We need to use the data set size to and cortest.barlett() function with the dataset size.
cortest.bartlett(cor_matrix, n=1300)
data$Attrition <- as.numeric(factor(data$Attrition))
data$BusinessTravel <- as.numeric(factor(data$BusinessTravel))
data$Department  <- as.numeric(factor(data$Department))
data$EducationField <- as.numeric(factor(data$EducationField))
data$Gender <- as.numeric(factor(data$Gender))
data$JobRole <- as.numeric(factor(data$JobRole))
data$MaritalStatus <- as.numeric(factor(data$MaritalStatus))
data$Over18 <- as.numeric(factor(data$Over18))
data <- read.csv("Employee.csv", header = TRUE, sep=",")
head(data)
data$Attrition <- as.numeric(factor(data$Attrition))
data$BusinessTravel <- as.numeric(factor(data$BusinessTravel))
data$Department  <- as.numeric(factor(data$Department))
data$EducationField <- as.numeric(factor(data$EducationField))
data$Gender <- as.numeric(factor(data$Gender))
data$JobRole <- as.numeric(factor(data$JobRole))
data$MaritalStatus <- as.numeric(factor(data$MaritalStatus))
data$Over18 <- as.numeric(factor(data$Over18))
data$OverTime <- as.numeric(factor(data$OverTime))
data$Attrition <- as.numeric(factor(data$Attrition))
head(data)
data <- as.data.frame(scale(data)) # we have to do this because it will give us atomic variables back if we do not.
head(data)
tail(data)
#sum(is.na(data.df)) # data values that have NA
sum(is.null(data)) # Data values that just have nothing in them
data <- subset(data, select = -c(9, 22, 27))
#See if there are any more NA values
sum(is.na(data))
head(data)
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
train <- subset(data, sample==TRUE)
test <- subset(data, sample == FALSE)
head(train)
# The variable jobLevel can be changed to anything that we want just make to sure to change the variables you want in the Training set as well.
model <- lm(JobLevel ~., data = train)
summary(model)
cor_matrix <- cor(data)
cov_matrix <- cov(data)
cor_matrix
cov_matrix
matrix_data <- data.matrix(data)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
## Look up what Barlet Test and KMO test Actually mean
## Find the Statistical Significance of the Barlet Test
# We need to use the data set size to and cortest.barlett() function with the dataset size.
cortest.bartlett(cor_matrix, n=1300)
det(cor_matrix)
# Explain what all of this means
det(cor_matrix)
# Explain what all of this means
# Load PCA related packages
library(stats)
library(modeldata)
library(dplyr)
library(psych)
library(rela)
library(MASS)
library(parallel)
library(creditmodel)
library(caTools)
data <- read.csv("Employee.csv", header = TRUE, sep=",")
head(data)
data$Attrition <- as.numeric(factor(data$Attrition))
data$BusinessTravel <- as.numeric(factor(data$BusinessTravel))
data$Department  <- as.numeric(factor(data$Department))
data$EducationField <- as.numeric(factor(data$EducationField))
data$Gender <- as.numeric(factor(data$Gender))
data$JobRole <- as.numeric(factor(data$JobRole))
data$MaritalStatus <- as.numeric(factor(data$MaritalStatus))
data$Over18 <- as.numeric(factor(data$Over18))
data$OverTime <- as.numeric(factor(data$OverTime))
data$Attrition <- as.numeric(factor(data$Attrition))
head(data)
data <- as.data.frame(scale(data)) # we have to do this because it will give us atomic variables back if we do not.
head(data)
tail(data)
#sum(is.na(data.df)) # data values that have NA
sum(is.null(data)) # Data values that just have nothing in them
data <- subset(data, select = -c(9, 22, 27))
#See if there are any more NA values
sum(is.na(data))
head(data)
sample <- sample.split(data$JobLevel, SplitRatio = 0.80) # split the dataset into 80% training and 20% for testing
train <- subset(data, sample==TRUE)
test <- subset(data, sample == FALSE)
head(train)
# The variable jobLevel can be changed to anything that we want just make to sure to change the variables you want in the Training set as well.
model <- lm(JobLevel ~., data = train)
summary(model)
cor_matrix <- cor(data)
cov_matrix <- cov(data)
cor_matrix
cov_matrix
matrix_data <- data.matrix(data)
pafData <- paf(matrix_data, eigcrit=1, convcrit = .001)
summary(pafData)
## Look up what Barlet Test and KMO test Actually mean
## Find the Statistical Significance of the Barlet Test
# We need to use the data set size to and cortest.barlett() function with the dataset size.
cortest.bartlett(cor_matrix, n=1300)
det(cor_matrix)
# Explain what all of this means
prinComponents <- prcomp(data) # we do need to use the scale because we have already scaled the data.
prinComponents
# Need to look up and write about what this is doing.
plot(prinComponents)
biplot(prinComponents)
# Need to look up and see what the different graphs mean.
screeplot(prinComponents)
biplot(prinComponents)
# Need to look up and see what the different graphs mean.
screeplot(prinComponents)
screeplot(prinComponents, type = "lines")
biplot(prinComponents)
# Need to look up and see what the different graphs mean.
pcacor <- cor(data)
pcacor
summary(pcacor)
# to find the variables we are calculating the all 6 components
pca6Components <- principal(pcacor, nfactors=6, rotate="none")
pca6Components
# to find the variables we are calculating the all 6 components
pca6Components <- principal(cor_matrix, nfactors=6, rotate="none")
pca6Components
# below we are finding the MR values of the pca component
test <- fa(cor_matrix, nfactors=6, rotate="none")
test
# below we are ploting the data plots for the better analysis.
alpha(pcacor, check.keys = TRUE)
fa.parallel(data,n.obs=1300)
fa.diagram(pca6Components)
# below we are ploting the data plots for the better analysis.
alpha(pcacor, check.keys = TRUE)
fa.parallel(data,n.obs=400)
fa.diagram(pca6Components)
prinComponents <- prcomp(data) # we do need to use the scale because we have already scaled the data.
summary(prinComponents)
# Need to look up and write about what this is doing.
prinComponents$sdev ^ 2
components <- cbind(JobLevel = data[, JobLevel], prinComponents$x[, 1:2]) %>% as.data.frame()
components <- cbind(data$JobLevel = data[, JobLevel], prinComponents$x[, 1:2]) %>% as.data.frame()
components <- cbind(data$JobLevel, prinComponents$x[, 1:2]) %>% as.data.frame()
components <- cbind(data$JobLevel, prinComponents$x[, 1:2]) %>% as.data.frame()
model2 <- lm(JobLevel ~., data = components)
components <- cbind(data$JobLevel, prinComponents$x[, 1:2]) %>% as.data.frame()
model2 <- lm(data$JobLevel ~., data = components)
components <- cbind(data$JobLevel, prinComponents$x[, 1:2]) %>% as.data.frame()
model2 <- lm(data$JobLevel ~., data = components)
model2
components <- cbind(data$JobLevel, prinComponents$x[, 1:2]) %>% as.data.frame()
model2 <- lm(data$JobLevel ~., data = components)
model2
summary(model)%adj.r.squared
components <- cbind(data$JobLevel, prinComponents$x[, 1:2]) %>% as.data.frame()
model2 <- lm(data$JobLevel ~., data = components)
model2
summary(model)$adj.r.squared
components <- cbind(data$JobLevel, prinComponents$x[, 1:2]) %>% as.data.frame()
model2 <- lm(data$JobLevel ~., data = components)
model2
summary(model)$adj.r.squared
summary(model2)$adj.r.squared
components <- cbind(data$JobLevel, prinComponents$x[, 1:5]) %>% as.data.frame()
model2 <- lm(data$JobLevel ~., data = components)
model2
summary(model)$adj.r.squared
summary(model2)$adj.r.squared
components <- cbind(data$JobLevel, prinComponents$x[, 1:14]) %>% as.data.frame()
model2 <- lm(data$JobLevel ~., data = components)
model2
summary(model)$adj.r.squared
summary(model2)$adj.r.squared
summary(model)$adj.r.squared
summary(model2)$adj.r.squared
knitr::opts_chunk$set(echo = TRUE)
library(edgeR)
library(edgeR)
install.packages("BiocManager")
library(edgeR)
library(edgeR)
library(edgeR)
if (!requireNamespace("BiocManager"))
install.packages("BiocManager")
BiocManager::install(c("limma", "edgeR", "Glimma", "org.Mm.eg.db", "gplots", "RColorBrewer", "NMF", "BiasedUrn"))
